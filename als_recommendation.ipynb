{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --max_line_length 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import timeit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.als_recommender import AlternatingLeastSquares\n",
    "from source import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = utils.create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "als = AlternatingLeastSquares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 50000...\n",
      "\t13.66 min\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in [50, 100, 150]:\n",
    "    print(f'Running {i*1000}...')\n",
    "\n",
    "    train, test = utils.prepare_data(spark, i*1000)\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    als.grid_search(train, rank=[10], max_iter=[5],\n",
    "                    reg_param=np.linspace(0.05, 0.3, 10), num_folds=2,\n",
    "                    metric='rmse', parallelism=2)\n",
    "    stop = timeit.default_timer()\n",
    "    runtime = stop - start\n",
    "    print(f'\\t{runtime/60 :.2f} min')\n",
    "\n",
    "    pred_ratings, pred_rankings = als.predict(test)\n",
    "    test_rmse = als.rmse()\n",
    "    test_precision = als.precision_at_k(k)\n",
    "\n",
    "    pd_pred_ratings = pred_ratings.toPandas()\n",
    "    pd_pred_rankings = pred_rankings.toPandas()\n",
    "\n",
    "    rmse_by_user = utils.rmse_distribution(pd_pred_ratings, 'userId')\n",
    "    rmse_by_movie = utils.rmse_distribution(pd_pred_ratings, 'movieId')\n",
    "\n",
    "    top_k_precision_by_user = utils.top_k_precision_distribution(pd_pred_rankings, k)\n",
    "    test_coverage = utils.calculate_coverage(pd_pred_rankings)\n",
    "\n",
    "    with open(f'data/results/als_{i}_dist_rmse_user.pkl', 'wb') as f:\n",
    "        pickle.dump([rmse_by_user, 'als_dist_rmse_user'], f)\n",
    "\n",
    "    with open(f'data/results/als_{i}_dist_rmse_movie.pkl', 'wb') as f:\n",
    "        pickle.dump([rmse_by_movie, 'als_dist_rmse_movie'], f)\n",
    "\n",
    "    with open(f'data/results/als_{i}_dist_topk_user.pkl', 'wb') as f:\n",
    "        pickle.dump([top_k_precision_by_user, 'als_dist_topk_user'], f)\n",
    "\n",
    "    experiment_title = 'als_{i}_hyper_rmse'\n",
    "    reg_params = [r['regParam'] for r in als.results]\n",
    "    rmses = [r['rmse'] for r in als.results]\n",
    "\n",
    "    result = [experiment_title, pd.Series(reg_params), pd.Series(rmses)]\n",
    "    with open(f'data/results/als_{i}_hyper_rmse_.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "\n",
    "    _, pred_rankings = als.predict(train)\n",
    "    train_rmse = als.rmse()\n",
    "    train_precision = als.precision_at_k(k)\n",
    "    pd_pred_rankings = pred_rankings.toPandas()\n",
    "    test_coverage = utils.calculate_coverage(pd_pred_rankings)\n",
    "\n",
    "    results.append({'sample_size': i,\n",
    "                    'runtime': runtime,\n",
    "                    'rmse_train': train_rmse,\n",
    "                    'rmse_test': test_rmse, \n",
    "                    'top_k_precision_train': train_precision,\n",
    "                    'top_k_precision_test': test_precision,\n",
    "                    'coverage_train': train_coverage,\n",
    "                    'coverage_test': test_coverage})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(results)\n",
    "result = result[['sample_size', 'runtime', 'rmse_train', 'rmse_test',\n",
    "                 'top_k_precision_train', 'top_k_precision_test']]\n",
    "experiment_title = f'result'\n",
    "with open(f'data/results/{experiment_title}.pkl', 'wb') as f:\n",
    "    pickle.dump(result, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
