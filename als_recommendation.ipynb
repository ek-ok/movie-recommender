{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%flake8_on --max_line_length 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import timeit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.als_recommender import AlternatingLeastSquares\n",
    "from source import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = utils.create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "als = AlternatingLeastSquares()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 50000...\n",
      "10.58 min\n",
      "{'sample_size': 50, 'runtime': 634.6269052030002, 'rmse_train': 0.7727328968582378, 'rmse_test': 0.9614838862055153, 'top_k_precision_train': 0.33837837837837903, 'top_k_precision_test': 0.503783783783784, 'coverage_train': 0.17253398396653885, 'coverage_test': 0.24398745207389333}\n",
      "\n",
      "Running 100000...\n",
      "11.85 min\n",
      "{'sample_size': 100, 'runtime': 710.8382729289997, 'rmse_train': 0.7930586364424529, 'rmse_test': 0.9262465358383585, 'top_k_precision_train': 0.29700427960057013, 'top_k_precision_test': 0.4987161198288162, 'coverage_train': 0.18527607361963191, 'coverage_test': 0.258159509202454}\n",
      "\n",
      "Running 150000...\n",
      "12.66 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29:1: W293 blank line contains whitespace\n",
      "32:1: W293 blank line contains whitespace\n",
      "62:1: W293 blank line contains whitespace\n",
      "65:1: W293 blank line contains whitespace\n",
      "66:1: W293 blank line contains whitespace\n",
      "67:5: E303 too many blank lines (2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sample_size': 150, 'runtime': 759.6038995669996, 'rmse_train': 0.786878858418734, 'rmse_test': 0.9150003754361845, 'top_k_precision_train': 0.29083585095669556, 'top_k_precision_test': 0.5057401812688828, 'coverage_train': 0.17997293640054127, 'coverage_test': 0.2526580320896965}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in [50, 100, 150]:\n",
    "    print(f'Running {i*1000}...')\n",
    "\n",
    "    train, test = utils.prepare_data(spark, i*1000)\n",
    "    pd_train, pd_test = train.toPandas(), test.toPandas()\n",
    "\n",
    "    start = timeit.default_timer()\n",
    "    als.grid_search(train, rank=[10], max_iter=[5],\n",
    "                    reg_param=np.linspace(0.05, 0.3, 10), num_folds=2,\n",
    "                    metric='rmse', parallelism=2)\n",
    "    stop = timeit.default_timer()\n",
    "    runtime = stop - start\n",
    "    print(f'{runtime/60 :.2f} min')\n",
    "\n",
    "    pred_ratings, pred_rankings = als.predict(test)\n",
    "    test_rmse = als.rmse()\n",
    "#     test_precision = als.precision_at_k(k)\n",
    "\n",
    "    pd_pred_ratings = pred_ratings.toPandas()\n",
    "    pd_pred_rankings = pred_rankings.toPandas()\n",
    "\n",
    "    rmse_by_user = utils.rmse_distribution(pd_pred_ratings, 'userId')\n",
    "    rmse_by_movie = utils.rmse_distribution(pd_pred_ratings, 'movieId')\n",
    "\n",
    "    pd_pred_rankings['userRanking'] = pd_pred_rankings.userRanking.apply(lambda x: x[:5])\n",
    "    pd_pred_rankings['predictedRanking'] = pd_pred_rankings.predictedRanking.apply(lambda x: x[:5])\n",
    "    \n",
    "    top_k_precision_by_user = utils.top_k_precision_distribution(pd_pred_rankings, k)\n",
    "    test_precision = top_k_precision_by_user.mean()\n",
    "    \n",
    "    test_coverage = utils.calculate_coverage(pd.merge(pd_test, pd_pred_rankings, on='userId'))\n",
    "\n",
    "    experiment_title = f'ALS_{i}_Dist_RMSE_User'\n",
    "    with open(f'data/results/{experiment_title}.pkl', 'wb') as f:\n",
    "        pickle.dump([rmse_by_user, experiment_title], f)\n",
    "\n",
    "    experiment_title = f'ALS_{i}_Dist_RMSE_Movie'\n",
    "    with open(f'data/results/{experiment_title}.pkl', 'wb') as f:\n",
    "        pickle.dump([rmse_by_movie, experiment_title], f)\n",
    "\n",
    "    experiment_title = f'ALS_{i}_Dist_Precision_Movie'\n",
    "    with open(f'data/results/{experiment_title}.pkl', 'wb') as f:\n",
    "        pickle.dump([top_k_precision_by_user, experiment_title], f)\n",
    "\n",
    "    experiment_title = f'ALS_{i}_Hyper_RMSE'\n",
    "    reg_params = [r['regParam'] for r in als.results]\n",
    "    rmses = [r['rmse'] for r in als.results]\n",
    "\n",
    "    result = [experiment_title, pd.Series(reg_params), pd.Series(rmses)]\n",
    "    with open(f'data/results/{experiment_title}.pkl', 'wb') as f:\n",
    "        pickle.dump(result, f)\n",
    "\n",
    "    _, pred_rankings = als.predict(train)\n",
    "    train_rmse = als.rmse()\n",
    "#     train_precision = als.precision_at_k(k)\n",
    "    pd_pred_rankings = pred_rankings.toPandas()\n",
    "\n",
    "    pd_pred_rankings['userRanking'] = pd_pred_rankings.userRanking.apply(lambda x: x[:5])\n",
    "    pd_pred_rankings['predictedRanking'] = pd_pred_rankings.predictedRanking.apply(lambda x: x[:5])\n",
    "    \n",
    "    top_k_precision_by_user = utils.top_k_precision_distribution(pd_pred_rankings, k)\n",
    "    train_precision = top_k_precision_by_user.mean()\n",
    "    \n",
    "    \n",
    "    train_coverage = utils.calculate_coverage(pd.merge(pd_train, pd_pred_rankings, on='userId'))\n",
    "\n",
    "    result = {'sample_size': i,\n",
    "              'runtime': runtime,\n",
    "              'rmse_train': train_rmse,\n",
    "              'rmse_test': test_rmse,\n",
    "              'top_k_precision_train': train_precision,\n",
    "              'top_k_precision_test': test_precision,\n",
    "              'coverage_train': train_coverage,\n",
    "              'coverage_test': test_coverage}\n",
    "    results.append(result)\n",
    "    print(result)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_pickle('data/results/ALS_result.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
